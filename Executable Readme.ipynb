{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project 3: Sparkify- Song Play DW and Analysis in AWS cloud environment \n",
    "\n",
    "<h1>Purpose of the Project</h1>\n",
    "\n",
    "<p>\n",
    "Sparkify is an online music startup that supports two types of users. First type of users belong to level \"free\", who are non-paying and second type of users belong to level \"paid\". Paid users generate the revenue for Sparkify and Free users are potential \"Paid\" users, who may subscribe to \"Paid Level\" in future. \n",
    "\n",
    "To support this business, it is important for Sparkify to collect correct data and metrics around users, songs, artists, usage patterns etc. This Project aims to create a data model that can store the required data, support business queries against the data, and can help calculate relevant metrics. With time volume of data/transactions has increased and Sparkify is now leverating AWS S3 for storage of raw data, and Redshift for Data Warehouse. It also is using Python for ETL and related programming.\n",
    "    \n",
    "</p>\n",
    "\n",
    "\n",
    "## Summary\n",
    "* [Data Model](#Data-Model)\n",
    "* [ETL Details](#ETL-Details)\n",
    "* [Project Artifacts](#Project-Artifacts)\n",
    "* [SQL Queries](#SQL-Queries)\n",
    "--------------------------------------------\n",
    "\n",
    "\n",
    "#### Data Model\n",
    "This is the datamodel of the DW, which is following a classical start schema methodology.\n",
    "The Fact Table is \"songplays\" which is inturn linked to dimension tables-\n",
    "Time, Users, Songs, Artists.\n",
    "\n",
    "\n",
    "![datamodel](DataModel.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "#### ETL Details\n",
    "\n",
    "\n",
    "<b> First we need to set up the environment: </b><br>\n",
    "Please follow the below steps and refer to notebook \"Setting Environment.ipynb\": <br>\n",
    "1. First we need to create an User with Admin privileges and save the credentials.<br>\n",
    "2. Next we need to prepare the configuration file with above credntials, and parameters to create cluster and connecting to it.<br>\n",
    "3. Next we create a 4 node, dc2 redshift cluster and let it become available. We create a database DWH with custom user and password.<br>\n",
    "4. We need to ensure during cluster creation that it is attached to right role with right privileges(read access to S3).\n",
    "5. We next note the ARN and Endpoint/Hostname.<br>\n",
    "6. We update the configuration file of step 2 with above details as those will be needed to connect to AWS-redshift.<br>\n",
    "\n",
    "\n",
    "<b> Next we can start DDL creation and ETL activities: </b><br>\n",
    "Open Terminal Session and  insert these commands in order to run the table creation and data load: <br><br>\n",
    "<I> Below will create the tables </I> <br>\n",
    "`` python create_tables.py`` <br>\n",
    "\n",
    "<I> Below will execute ETL process of loading the staging tables first from JSON log files in S3 and will then load DW tables </I> <br>\n",
    "<I> This step also prints the total execution time in seconds.</I> \n",
    "<br>\n",
    "`` python etl.py`` <br>\n",
    "\n",
    "\n",
    "![terminal](terminal_screenshot.jpg)\n",
    "----------------------------\n",
    "\n",
    "#### Project Artifacts\n",
    "\n",
    "* <b> create_tables.py </b> - This script will drop old tables (if exist) ad re-create new tables\n",
    "* <b> etl.py </b> - This script will read JSON every file contained in /data folder, parse them, <br> build relations though logical process and ingest data \n",
    "* <b> sql_queries.py </b> - This file contains variables with SQL and COPY statements needed for the project in String formats, <br> \n",
    "* <b> Setting Environment.ipynb </b> - This notebook creates the cluster, and runs SQLs after data is loaded\n",
    "* <b> Executable Readme.ipynb </b>- This notebook, execute the markdown file to give a look and feel of final output of the md-file.\n",
    "----------------------------\n",
    "\n",
    "#### SQL Queries\n",
    "\n",
    "<I> Getting Most Used User Agent:</I>\n",
    "``` SQL\n",
    "     SELECT user_agent from \n",
    "    (  SELECT user_agent, count(user_agent) cnt_user_agent FROM songplays GROUP BY user_agent )abc \n",
    "    where cnt_user_agent in \n",
    "    ( \n",
    "    SELECT MAX(cnt_user_agent) max_cnt_user_agent from \n",
    "    ( SELECT user_agent, count(user_agent) cnt_user_agent FROM songplays GROUP BY user_agent)\n",
    "    abc\n",
    "    );\n",
    "```\n",
    "\n",
    "<I> Getting the busiest hours, that is the hours when most songs are being played:</I>\n",
    "``` SQL\n",
    "    SELECT hour, count(songplay_id) cnt_plays from \n",
    "    songplays a inner join time b \n",
    "    on a.start_time=b.start_time \n",
    "    group by hour \n",
    "    order by cnt_plays desc LIMIT 5; \n",
    "```\n",
    "\n",
    "<I> Average Daily Active Users:</I>\n",
    "``` SQL\n",
    "    SELECT tot_daily_usr_cnt/num_days \n",
    "    from \n",
    "    (\n",
    "    SELECT SUM(daily_usr_cnt) tot_daily_usr_cnt from         \n",
    "    (SELECT day,month,year, count(distinct(user_id)) daily_usr_cnt\n",
    "    from songplays a\n",
    "    INNER join time b \n",
    "    on a.start_time=b.start_time \n",
    "    group by day,month, year \n",
    "    )a\n",
    "    )c    \n",
    "    INNER join  \n",
    "     (\n",
    "     SELECT COUNT(*) num_days from \n",
    "     \n",
    "    (SELECT day,month,year\n",
    "     from songplays a \n",
    "     INNER join time b\n",
    "     on a.start_time=b.start_time\n",
    "      GROUP BY day, month, year) b \n",
    "       )b1\n",
    "     on (1=1);\n",
    "```\n",
    "\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
